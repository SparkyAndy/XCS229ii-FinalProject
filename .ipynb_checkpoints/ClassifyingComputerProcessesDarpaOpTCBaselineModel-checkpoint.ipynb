{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.8\n"
     ]
    }
   ],
   "source": [
    "# library imports\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The DARPA OpTC dataset\n",
    "\n",
    "The DARPA OpTC dataset contains events from an isolated enterprise network of 1000 host computers. Sensors logged ‘action’ events associated with 11 ‘object’ types over 14 days. On 3 consecutive days, a ‘red team’ introduced malware and malicious activity on 29 computers (1).\n",
    "\n",
    "The dataset is hosted on a Google drive (2) and documentation is available in a Github repository (2). A ground truth document (4) provides details of the malicious events. Note that we define a malicious process to be a process which is linked to a malicious event detailed in the ground truth (4). The starting point for the project was the raw json data (3), ground truth table (4) and a table of ‘process create’ event data (extracted from properties in the raw json `PROCESS_CREATE_ events).\n",
    "\n",
    "The DARPA OpTC dataset has over 17 billion events: significant data engineering is required to create the feature vectors for the Machine Learning (ML) experiments. \n",
    "\n",
    "We will establish a simple and interpretable baseline for the classification of computer process activity in the DARPA OpTC dataset. For each process, we will aggregate counts for the observed set of (object, action) events. Feature vectors for Machine Learning (ML) will be created by joining the frequency counts with ‘ground truth’ labels. \n",
    "\n",
    "## Extract Transform Load (ETL) data engineering pipeline for Machine Learning (ML) data\n",
    "\n",
    "The raw json data was downloaded to AWS S3 and partitioned into 4554 part files, each o(500MB).\n",
    "\n",
    "In the first ETL stage, we selected the data for the 29 computers attacked on the 3 ‘red team’ days, dropping detailed meta-data contained in ‘properties’ tables (1, 2, 4). The resulting 228 ‘summary’ files, each o(400MB), contained the following fields:\n",
    "\n",
    "- object – one of {FILE, FLOW, HOST, MODULE, PROCESS, REGISTRY, SERVICE, SHELL, TASK, THREAD, USER_SESSION}\n",
    "\n",
    "- action – one of a limited set associated with a given object: see Figure 1 below for types\n",
    "\n",
    "- actorID – the unique identifier (UID) for the parent process\n",
    "\n",
    "•\tobjectID – the unique identifier (UID) for the object (child process if object = PROCESS)\n",
    "\n",
    "•\thostname – the name of the computer (eg. SysClient0501.systemia.com)\n",
    "\n",
    "In the next stage, we created aggregate counts for every object.action pair observed for each actorID. The resulting 212 ‘aggregate’ files, each o(400KB), contained following additional field:\n",
    "\n",
    "•\tobject.action counts – dictionary of counts for the observed object.action pairs\n",
    " \n",
    "In the final ETL stage, we join reference data from the ground truth table (4, 5) and the ‘process create’ table to the ‘aggregate’ data to create a dataframe of feature vectors for Machine Learning (ML). The resulting ML dataframe has shape (349841, 37). The first 4 columns in the ML dataframe are\n",
    "\n",
    "•\tactorID – the unique identifier (UID) for the parent process\n",
    "\n",
    "•\thostname – the name of the computer (eg. SysClient0501.systemia.com)\n",
    "\n",
    "•\tprocess_name – the name of the executable (eg. Powershell.exe)\n",
    "\n",
    "•\tlabel – 0 for benign and 1 for malicious. \n",
    "\n",
    "The remaining 33 columns contain the object.action counts for actorID. \n",
    "\n",
    "## REFERENCES\n",
    "\n",
    "(1) Md. Monowar Anjum, Shahrear Iqbal and Benoit Hamelin. 2021. Analysing the Usefulness of the DARPA OpTC Dataset in Cyber Threat Detection Research. arXiv:2103.03080v2. Retrieved from https://arxiv.org/abs/2103.03080 \n",
    "Accepted for ACM Symposium on Access Control Models and Technologies (SACMAT), 16-18 June, 2021, Barcelona, Spain (virtual event). ACM Inc., New York, NY. DOI: https://doi.org/10.1145/3450569.3463573\n",
    "\n",
    "(2) DARPA. 2020. Operationally Transparent Cyber (OpTC) Data Release. README. Retrieved from http://github.com/FiveDirections/OpTC-data\n",
    "\n",
    "(3) DARPA. 2020. Operationally Transparent Cyber (OpTC) Data Release. Retrieved from https://drive.google.com/drive/u/0/folders/1n3kkS3KR31KUegn42yk3-e6JkZvf0Caa\n",
    "\n",
    "(4) DARPA. 2020. OpTC Red Team Ground Truth. Retrieved April 7, 2021 from https://github.com/FiveDirections/OpTC-data/blob/master/OpTCRedTeamGroundTruth.pdf\n",
    "\n",
    "(5) labels.csv retrieved from (need approval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datafile = \"C:/Users/arvea/Downloads/darpa_data_revised.csv\"\n",
    "dftest = pd.read_csv(datafile, index_col=0)\n",
    "print(dftest.shape)\n",
    "print(dftest.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check actorID is unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(dftest['actorID'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class imbalance\n",
    "\n",
    "Only 0.0016% of the *events* in the complete dataset (1) are malicious, so it is marginal whether there are sufficient examples for supervised learning.\n",
    "\n",
    "We deine a *malicious process* as a process that is linked to a *malicious event* in the ground truth- that does NOT mean all events associated with a *malicious process* are malicious!\n",
    "\n",
    "Let's get the class imbalance in our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dftest['label'].value_counts())\n",
    "total = dftest.shape[0]\n",
    "total0 = dftest['label'].value_counts()[0]\n",
    "total1 = dftest['label'].value_counts()[1]\n",
    "print(\"probBenign: \", float(total0)/total, \", probMalicious: \", float(total1)/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of event types in the dataframe - careful interpretting it \n",
    "\n",
    "Remember the class imbalance when interpretting this figure\n",
    "\n",
    "The fraction of event types for benign and malicious processes in the Machine Learning (ML) dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall frequency of (object.action) pairs\n",
    "X = dftest.iloc[:, 4:]\n",
    "feature_list = list(X.columns)\n",
    "frequencies = X.sum(axis=0)\n",
    "\n",
    "X0 = dftest[dftest['label'] == 0].iloc[:, 4:]\n",
    "X1 = dftest[dftest['label'] == 1].iloc[:, 4:]\n",
    "\n",
    "frequencies0 = X0.sum(axis=0)\n",
    "frequencies1 = X1.sum(axis=0)\n",
    "\n",
    "total0 = frequencies0.sum()\n",
    "total1 = frequencies1.sum()\n",
    "\n",
    "fraction0 = (frequencies0 * 100.0) / total0\n",
    "fraction1 = (frequencies1 * 100.0) / total1\n",
    "\n",
    "total1\n",
    "\n",
    "%matplotlib inline\n",
    "# Set the style\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "# Make a bar chart\n",
    "#plt.bar(x_values, fraction1, orientation = 'vertical')\n",
    "# Tick labels for x axis\n",
    "#plt.xticks(x_values, feature_list, rotation='vertical')\n",
    "\n",
    "# labels - feature_list\n",
    "# fraction0 - benign\n",
    "# fraction1 - malicious\n",
    "\n",
    "x_values = np.arange(len(fraction1))  # list of label locations for plotting\n",
    "width = 0.35                            # width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects0 = ax.bar(x_values - width/2, fraction0, width, label='Benign')\n",
    "rects1 = ax.bar(x_values + width/2, fraction1, width, label='Malicious')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Fraction - %')\n",
    "ax.set_xlabel('Event type - object.action')\n",
    "ax.set_title('Fraction of event types observed for benign and malicious processes')\n",
    "ax.set_xticks(x_values)\n",
    "ax.set_xticklabels(feature_list, Rotation = 'vertical')\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model - Random Forest \n",
    "\n",
    "> \"The results are not bad when you consider the enormity of the swindle which I have perpetrated!\"\n",
    "Lars Onsager Biographical Memoirs of Fellows of the Royal Society 24 443\n",
    "\n",
    "## Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (2nd Edition)\n",
    "\n",
    "Superb practical book on Machine Learning with Scikit_Learn (6)\n",
    "\n",
    "https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/\n",
    "\n",
    "There is a github repo for the book\n",
    "\n",
    "https://github.com/ageron\n",
    "\n",
    "https://github.com/ageron/handson-ml2\n",
    "\n",
    "There are Jupyter notebooks for each chapter of the book - I have used some as starter code (Precision - Recall curves)\n",
    "\n",
    "https://github.com/ageron/handson-ml2/blob/master/03_classification.ipynb\n",
    "\n",
    "## Machine Learning with Python Cookbook\n",
    "\n",
    "Another practical book with code examples (7)\n",
    "\n",
    "https://www.oreilly.com/library/view/machine-learning-with/9781491989371/\n",
    "\n",
    "\n",
    "## Scikit-Learn RandomForestClassifier\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "Parameters that are set for RandomForestClassifier (7 - 14.4 Training a Random Forest Classifier)\n",
    "\n",
    "* random_state = 42 -> (because we like Hitch Hikers Guide to the Galaxy)\n",
    "\n",
    "* class_weight = 'balanced' -> mitigates the class imbalance (7 - 14.8 Handling Imbalanced Classes)\n",
    "\n",
    "* oob_score - 'True' -> gives you Out Of Bag score (accuracy) (7 - 14.11 Evaluating Random Forests with Out-of-Bag Errors)\n",
    "\n",
    "* n_estimators = default (100) -> will do hyper parameter tuning later\n",
    "\n",
    "* n_jobs=-1 -> use all cores (7 - )\n",
    "\n",
    "## Scikit-Learn Train, Test Split - initial naive stratifed sampling approach\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "IMPORTANT - parameters for train_test_split:\n",
    "\n",
    "* stratify=y -> maintain proportion of class labels in train and test\n",
    "\n",
    "* test_size= 0.3 -> old fashioned 70:30 split for small data: review this\n",
    "\n",
    "* random_state=42 -> (Hitch Hiker ...)\n",
    "\n",
    "COMMENT - we will own up to the 'swindle' in good time - how have we split into train / test: random stratified\n",
    "\n",
    "\n",
    "## METRICS\n",
    "\n",
    "\n",
    "\n",
    "Accuracy - optimised by Random Forest - but cost weighting and stratification help mitigate class imbalance\n",
    "\n",
    "Confusion matrix - TN, FP, FN, TP - can work out most of required metrics from these, which we do in this cell\n",
    "\n",
    "Should consider visualization using scikit-learn functions\n",
    "\n",
    "Precison, Recall, F1 score - our key metrics - we will optimize using F1 score (Andrew Ng - have ONE score)\n",
    "\n",
    "Out of Bag estimate for accuracy - a good estimate of cross-validation score\n",
    "\n",
    "## REFERENCES\n",
    "\n",
    "(6) Aurelien Geron. 2019. Geron. _Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems_ O'Reilly Media, Inc. CA\n",
    "\n",
    "(7) Chris Albon. 2018. _Machine Learning with Python Cookbook: Practical Solutions from Preprocessing to Deep Learning_ O'Reilly Media, Inc. CA\n",
    "\n",
    "(8) Andrew Ng. 2018. _Machine Learning Yearning: Technical Strategy for AI Engineers, In the Era of Deep Learning. Draft Version._ Retrieved May 6, 2021 from https://www.deeplearning.ai/programs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Baseline model - Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, log_loss\n",
    "\n",
    "X = dftest.iloc[:, 4:]\n",
    "y = list(dftest['label'].values)   # may be better to keep dataframe - can always use dftest['label']\n",
    "\n",
    "# train, test split - stratified\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)\n",
    "\n",
    "clf = RandomForestClassifier(random_state = 42, class_weight = 'balanced', oob_score=True, n_jobs=-1)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_train_predict = clf.predict(X_train)\n",
    "y_test_predict = clf.predict(X_test)\n",
    "\n",
    "# replace the home brew with sklearn methods\n",
    "\n",
    "print(\"Training results:\")\n",
    "\n",
    "tn, fp, fn, tp =  confusion_matrix(y_train, y_train_predict).ravel()\n",
    "print(\"TN: \", tn, \", FP: \", fp, \", FN: \", fn, \", TP: \", tp)\n",
    "\n",
    "print(\"accuracy: \", accuracy_score(y_train, y_train_predict))\n",
    "print(\"precision: \", precision_score(y_train, y_train_predict))\n",
    "print(\"recall: \", recall_score(y_train, y_train_predict))\n",
    "print(\"F1 score: \", f1_score(y_train, y_train_predict))\n",
    "print(\"log loss: \", log_loss(y_train, y_train_predict))\n",
    "\n",
    "print(\"\\nOut of bag estimate - validation\")\n",
    "print(\"out of bag (accuracy) score: \", clf.oob_score_)\n",
    "\n",
    "print(\"\\nTest results:\")\n",
    "\n",
    "tn, fp, fn, tp =  confusion_matrix(y_test, y_test_predict).ravel()\n",
    "print(\"TN: \", tn, \", FP: \", fp, \", FN: \", fn, \", TP: \", tp)\n",
    "\n",
    "print(\"accuracy: \", accuracy_score(y_test, y_test_predict))\n",
    "print(\"precision: \", precision_score(y_test, y_test_predict))\n",
    "print(\"recall: \", recall_score(y_test, y_test_predict))\n",
    "print(\"F1 score: \", f1_score(y_test, y_test_predict))\n",
    "print(\"log loss: \", log_loss(y_test, y_test_predict))\n",
    "\n",
    "# create confusion matrix \n",
    "matrix = confusion_matrix(y_test, y_test_predict)\n",
    "\n",
    "# Create list of target class names - benign (0), malicious (1)\n",
    "\n",
    "class_names = ['benign', 'malicious']\n",
    "\n",
    "dataframe = pd.DataFrame(matrix, index=class_names, columns=class_names)\n",
    "\n",
    "# Create heatmap\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "%matplotlib inline\n",
    "# Set the style\n",
    "sns.heatmap(dataframe, annot=True, cbar=None, cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.ylabel(\"Predicted Class\", fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance and Feature Selection for Random Forest\n",
    "\n",
    "## Important - explanatory aspect of Random Forest is critical for cybersecurity\n",
    "\n",
    "## Machine Learning with Python Cookbook¶\n",
    "\n",
    "I've used code from _Machine Learning with Python Cookbook_ (7) - I am a nerd and have it on my smartphone Kindle app ...\n",
    "\n",
    "**14.6** _Identifying Important Features in Random Forests_ has useful starter code\n",
    "\n",
    "https://www.oreilly.com/library/view/machine-learning-with/9781491989371/\n",
    "\n",
    "https://learning.oreilly.com/library/view/machine-learning-with/9781491989371/ch14.html#trees-and-forests\n",
    "\n",
    "## Feature Selection choices\n",
    "\n",
    "Take top-20 features for some future experiments?\n",
    "\n",
    "Makes sense if you look at the figure below - and SHELL_COMMAND_ is ranked 19 :-)\n",
    "\n",
    "Why did Turing / Oxford (9) pick 20 features?\n",
    "\n",
    "Probably because there are 20 in the labels.csv Ground Truth (they don't say why?)\n",
    "\n",
    "## REFERENCES\n",
    "\n",
    "(9) Thomas Cochrane, Peter Foster, Varun Chhabra, Maud Lemercier, Terry Lyons and Cristopher Salvi. 2021. SK-Tree: a systematic malware detection algorithm on streaming trees via the signature kernel. arXiv:2102.07904v3. Retrieved from https://arxiv.org/abs/2102.07904"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature Importance\n",
    "\n",
    "# Credit Towards Data Science Medium post and OReilly REF\n",
    "\n",
    "# Random Forest feature importance - from cells above run CLF = RF ... maybe have optimized hyper-parameters\n",
    "\n",
    "feature_list = list(X.columns)\n",
    "importances = list(clf.feature_importances_)\n",
    "\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 8)) for feature, importance in zip(feature_list, importances)]\n",
    "\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:25} Importance: {}'.format(*pair)) for pair in feature_importances];\n",
    "\n",
    "# ********************* Figure showing feature importance *****************************\n",
    "\n",
    "%matplotlib inline\n",
    "# Set the style\n",
    "plt.style.use('seaborn')\n",
    "plt.tight_layout()\n",
    "# list of x locations for plotting\n",
    "x_values = list(range(len(importances)))\n",
    "# Make a bar chart\n",
    "plt.bar(x_values, importances, orientation = 'vertical')\n",
    "# Tick labels for x axis\n",
    "plt.xticks(x_values, feature_list, rotation='vertical')\n",
    "# Axis labels and title\n",
    "plt.ylabel('Importance'); plt.xlabel('Feature (object.action)'); plt.title('Random Forest Feature Importance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Precision - Recall curve\n",
    "\n",
    "StratifiedKFold(n_splits=5)\n",
    "\n",
    "cross_val_predict -> setn_jobs=-1 (use all cores)\n",
    "\n",
    "RandomForest has predict_proba\n",
    "\n",
    "## Geron _Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition_\n",
    "\n",
    "https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/\n",
    "\n",
    "Github repo\n",
    "\n",
    "https://github.com/ageron/handson-ml\n",
    "\n",
    "I've adapted code from this notebook:\n",
    "\n",
    "https://github.com/ageron/handson-ml/blob/master/03_classification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF PR curve\n",
    "\n",
    "# want cross_val_predict to be stratified version - does use stratified when set number - can pass in kfold\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "\n",
    "skfolds = StratifiedKFold(n_splits=5)  # no random shuffle as need to ensure cover all points for predict\n",
    "\n",
    "# cv=5 - uses startifed by default\n",
    "y_probas_forest = cross_val_predict(clf, X_train, y_train, cv=skfolds,\n",
    "                                    method=\"predict_proba\", n_jobs=-1)\n",
    "\n",
    "y_scores_forest = y_probas_forest[:, 1] # score = proba of positive class\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.metrics import roc_auc_score, auc\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train, y_scores_forest)\n",
    "\n",
    "AUCPR = auc(recalls, precisions)\n",
    "\n",
    "# average_precision = average_precision_score(y_train, y_scores_forest)\n",
    "\n",
    "threshold_value = 0.9146005509641874 # 0.7999789288419409 # RF setting *** MAX F1 score\n",
    "recall_value_precision = recalls[np.argmax(precisions >= threshold_value)]\n",
    "\n",
    "Max_F1_score = 2*(threshold_value*recall_value_precision)/(threshold_value+recall_value_precision)\n",
    "\n",
    "print(\"threshold value for precision: \", threshold_value)\n",
    "print(\"recall_value_threshold_precision: \", recall_value_precision)\n",
    "print(\"max FI score: \", Max_F1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics - Precision, Recall and F1 score\n",
    "\n",
    "In the figure below we highlight the point on the curve where the F1 score is maximized:\n",
    "\n",
    "- Remember that our core hypothesis is that we can distinguish between malicious and benign processes using only the ferquency count of the (object, action) events associated with each process as a feature vector.\n",
    "\n",
    "- We can see that we can recover 50% of the malicious processes (Recall) and of those 50%, 90% are true positives (Precision) without any tuning of hyperparameters\n",
    "\n",
    "- that suggests our hypothesis is reasonable\n",
    "\n",
    "- we give the Area Under Curve (AUC) for the Precison Recall curve - it is a useful metric but should be handled with care\n",
    "\n",
    "## Area under the curve - AUROC considered harmful (David Hands keynote at KDD 2009 (Paris, France))\n",
    "\n",
    "For imbalanced classes it is dubious to use the ROC curve and unforgiveable to use AUROC as a score, especially for optimization: I was at David Hand's KDD 2009 keynote and you could see demoralized graduate students after the talk ...\n",
    "\n",
    "I've also heard him give the same message at a workshop at Imperial College many years later - why is this not widely known?!?\n",
    "\n",
    "(10) David J. Hand. 2009. Mismatched Models, Wrong Results, and Dreadful Decisions. Keynote at 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), June, 2009 Paris, France recorded June 2009, published September 14, 2009 http://videolectures.net/kdd09_hand_mmwrdd/  (video) http://videolectures.net/site/normal_dl/tag=45840/kdd09_hand_mmwrdd_01.pdf (slides)\n",
    "\n",
    "(11) David J. Hand. 2009. Measuring classifier performance: a coherent alternative to the area under the ROC curve. Mach Learn, 77 (2009), 103–123. DOI: https://doi.org/10.1007/s10994-009-5119-5\n",
    "\n",
    "(12) Edsger Dijkstra (March 1968). \"Go To Statement Considered Harmful\". Communications of the ACM. 11 (3): 147–148. doi:10.1145/362929.362947. The unbridled use of the go to statement has as an immediate consequence that it becomes terribly hard to find a meaningful set of coordinates in which to describe the process progress. ... The go to statement as it stands is just too primitive, it is too much an invitation to make a mess of one's program.\n",
    "\n",
    "Fun fact or urban myth - Dijkstra refused to take graduate students who had programmed in FORTAN because it had a goto statement (true or false)\n",
    "\n",
    "My Literature Review turned up papers using AUROC - the worst example is (13) which is supposed to be about tackling class imbalance.\n",
    "\n",
    "(13) Charles Wheelus, Elias Bou-Harb and Xingquan Zhu. 2018. Tackling Class Imbalance in Cyber Security Datasets. In Proceedings of the 2018 IEEE International Conference on Information Reuse and Integration (IRI), 6-9 July, 2018, Salt Lake City, UT, USA. IEEE Xplore, 229-232. https://doi.org/10.1109/IRI.2018.00041"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_precision_vs_recall(precisions, recalls, clf, colour):\n",
    "    plt.plot(recalls, precisions, color=colour, alpha=0.7, linewidth=2, label=clf)\n",
    "    plt.fill_between(recalls, precisions, color=colour, alpha=0.3)\n",
    "    plt.title(\"Precision-Recall curve - AUC = {0:0.4f}\".format(AUCPR), fontsize=16)\n",
    "    plt.xlabel(\"Recall\", fontsize=16)\n",
    "    plt.ylabel(\"Precision\", fontsize=16)\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.axis([-0.05, 1.05, -0.05, 1.05])\n",
    "    plt.grid(True)\n",
    "\n",
    "%matplotlib inline\n",
    "# Set the style\n",
    "plt.style.use('seaborn')\n",
    "plt.tight_layout()\n",
    "plt.figure(figsize=(4, 4))\n",
    "plot_precision_vs_recall(precisions, recalls, 'Random Forest', 'b')\n",
    "plt.plot([recall_value_precision, recall_value_precision], [0., threshold_value], \"r:\")\n",
    "plt.plot([0.0, recall_value_precision], [threshold_value, threshold_value], \"r:\")\n",
    "plt.plot([recall_value_precision], [threshold_value], \"ro\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful explanatory plot adapted from Geron notebook\n",
    "\n",
    "## Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition¶\n",
    "\n",
    "https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/\n",
    "\n",
    "Github repo\n",
    "\n",
    "https://github.com/ageron/handson-ml\n",
    "\n",
    "I've adapted code from this notebook:\n",
    "\n",
    "https://github.com/ageron/handson-ml/blob/master/03_classification.ipynb\n",
    "\n",
    "## AUROC considered harmful\n",
    "\n",
    "I thought you said AUROC was harmful? \n",
    "\n",
    "This is provided for information only as it is prevalent in the literature and may be the only way to compare your results with their results.\n",
    "\n",
    "Shouldn't papers be rejected if they use AUROC for imbalanced class datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    plt.title(\"Precision, Recall & F1 Score vs Threshold\", fontsize=16)\n",
    "    plt.plot(thresholds, precisions[:-1], \"b-\", label=\"Precision\", linewidth=2)\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
    "    F1score = [2*(precision*recall)/(precision+recall) for precision, recall in zip(precisions, recalls)]\n",
    "    plt.plot(thresholds, F1score[:-1], \"c-\", label=\"F1 score\", linewidth=2)\n",
    "    plt.legend(loc=\"lower right\") # Not shown in the book\n",
    "    plt.xlabel(\"Threshold\", fontsize=16)        # Not shown\n",
    "    plt.ylabel(\"Precision, Recall, F1 Score\", fontsize=16)\n",
    "    plt.grid(True)                              # Not shown\n",
    "    plt.axis([0.6, 1.0, -0.05, 1.05])             # Not shown\n",
    "\n",
    "recall_value_precision = recalls[np.argmax(precisions >= threshold_value)]\n",
    "threshold_value_precision = thresholds[np.argmax(precisions >= threshold_value)]\n",
    "\n",
    "%matplotlib inline\n",
    "# Set the style\n",
    "plt.style.use('seaborn')\n",
    "plt.tight_layout()\n",
    "plt.figure(figsize=(6, 6))                                                                  # Not shown\n",
    "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
    "plt.plot([threshold_value_precision, threshold_value_precision], [0., threshold_value], \"r:\")                 # Not shown\n",
    "plt.plot([-0.05, threshold_value_precision], [threshold_value, threshold_value], \"r:\")                                # Not shown\n",
    "plt.plot([-0.05, threshold_value_precision], [recall_value_precision, recall_value_precision], \"r:\")# Not shown\n",
    "plt.plot([threshold_value_precision], [threshold_value], \"ro\")                                             # Not shown\n",
    "plt.plot([threshold_value_precision], [recall_value_precision], \"ro\")                             # Not shown\n",
    "plt.show()\n",
    "\n",
    "print(\"threshold_value_precision = \", threshold_value_precision)\n",
    "print(\"AUROC: \", roc_auc_score(y_train, y_scores_forest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjusting the threshold to maximize F1 score\n",
    "\n",
    "## Resulting Precision and Recall\n",
    "\n",
    "We waved hands a bit when we looked at the Figures above - what is the maxvalue of F1 and what are the Precision and Recall when you optimize the probability threshold?\n",
    "\n",
    "This could be argued to be premature optimization - we haven't done any hyperparameter tuning.\n",
    "\n",
    "Probably need to look at the code - the figures are probably not quite right - make sure you get the right threshold value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1score = [2*(precision*recall)/(precision+recall) for precision, recall in zip(precisions, recalls)]\n",
    "\n",
    "MaxF1Score = max(F1score)\n",
    "index = np.argmax(F1score)\n",
    "\n",
    "print(\"Maximum value of F1 score = \", MaxF1Score)\n",
    "print(\"Precision with optimized probability threshold = \", precisions[index])\n",
    "print(\"Recall with optimized probability threshold = \", recalls[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "threshold = threshold_value_precision  # threshold_value_precision\n",
    "\n",
    "predicted_proba = clf.predict_proba(X_test)\n",
    "predicted = (predicted_proba [:,1] >= threshold).astype('int')\n",
    "\n",
    "#prob_preds = clf.predict_proba(X)\n",
    "#threshold = 0.11 # define threshold here\n",
    "#preds = [1 if prob_preds[i][1]> threshold else 0 for i in range(len(prob_preds))]\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, log_loss\n",
    "\n",
    "print(\"accuracy: \", accuracy_score(y_test, predicted))\n",
    "print(\"precision: \", precision_score(y_test, predicted))\n",
    "print(\"recall: \", recall_score(y_test, predicted))\n",
    "print(\"F1 score: \", f1_score(y_test, predicted))\n",
    "print(\"log loss: \", log_loss(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic hyper parameter tuning for RandomForest\n",
    "\n",
    "## n_estimators\n",
    "\n",
    "Just use standard _stratfied_ cross validation with simple train/validation split\n",
    "\n",
    "Validation curves for **n_estimators** hyper parameter first\n",
    "\n",
    "Want to avoid over-fitting - listen to Andrew Ng!\n",
    "\n",
    "## Machine Learning with Python Cookbook\n",
    "\n",
    "**11.3** _Visualizing the Effect of Hyperparameter Values_ (7)\n",
    "\n",
    "https://www.oreilly.com/library/view/machine-learning-with/9781491989371/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "X = dftest.iloc[:, 4:]\n",
    "y = list(dftest['label'].values)   # may be better to keep dataframe - can always use dftest['label']\n",
    "\n",
    "# train, test split - stratified - LOWER DOWN!\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)\n",
    "\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skfolds = StratifiedKFold(n_splits=5)\n",
    "\n",
    "clf = RandomForestClassifier(random_state = 42, class_weight = 'balanced', oob_score=True, n_jobs=-1)\n",
    "\n",
    "# Create range of values for parameter\n",
    "param_range = np.arange(0, 200, 10)\n",
    "\n",
    "# Calculate accuracy on training and test set using range of parameter values\n",
    "train_scores, test_scores = validation_curve(\n",
    "    # Classifier\n",
    "    clf,\n",
    "    # Feature matrix\n",
    "    X_train,\n",
    "    # Target vector\n",
    "    y_train,\n",
    "    # Hyperparameter to examine\n",
    "    param_name=\"n_estimators\",\n",
    "    # Range of hyperparameter's values\n",
    "    param_range=param_range,\n",
    "    # Number of folds or the KFold object\n",
    "    cv=skfolds,\n",
    "    # Performance metric\n",
    "    scoring=\"accuracy\",\n",
    "    # Use all computer cores\n",
    "    n_jobs=-1)\n",
    "\n",
    "# Calculate mean and standard deviation for training set scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "\n",
    "# Calculate mean and standard deviation for test set scores\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plot mean accuracy scores for training and test sets\n",
    "plt.plot(param_range, train_mean, label=\"Training score\", color=\"black\")\n",
    "plt.plot(param_range, test_mean, label=\"Cross-validation score\", color=\"dimgrey\")\n",
    "\n",
    "# Plot accurancy bands for training and test sets\n",
    "plt.fill_between(param_range, train_mean - train_std,\n",
    "                 train_mean + train_std, color=\"gray\")\n",
    "plt.fill_between(param_range, test_mean - test_std,\n",
    "                 test_mean + test_std, color=\"gainsboro\")\n",
    "\n",
    "# Create plot\n",
    "plt.title(\"Validation Curve With Random Forest\")\n",
    "plt.xlabel(\"Number Of Estimators\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "plt.tight_layout()\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic hyper parameter tuning for RandomForest\n",
    "\n",
    "## max_depth\n",
    "\n",
    "Just use standard _stratfied_ cross validation with simple train/validation split\n",
    "\n",
    "Validation curves for **max_depth** hyper parameter second\n",
    "\n",
    "This is the key hyperparameter to set if we want to avoid over-fitting to the training set\n",
    "\n",
    "Want to avoid over-fitting - listen to Andrew Ng!\n",
    "\n",
    "## Machine Learning with Python Cookbook\n",
    "\n",
    "**11.3** _Visualizing the Effect of Hyperparameter Values_ (7)\n",
    "\n",
    "https://www.oreilly.com/library/view/machine-learning-with/9781491989371/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skfolds = StratifiedKFold(n_splits=5)\n",
    "\n",
    "clf = RandomForestClassifier(random_state = 42, class_weight = 'balanced', oob_score=True, n_jobs=-1)\n",
    "\n",
    "# Create range of values for parameter\n",
    "param_range = np.arange(0, 30, 2)\n",
    "\n",
    "# Calculate accuracy on training and test set using range of parameter values\n",
    "train_scores, test_scores = validation_curve(\n",
    "    # Classifier\n",
    "    clf,\n",
    "    # Feature matrix\n",
    "    X_train,\n",
    "    # Target vector\n",
    "    y_train,\n",
    "    # Hyperparameter to examine\n",
    "    param_name=\"max_depth\", #  \"n_estimators\",\n",
    "    # Range of hyperparameter's values\n",
    "    param_range=param_range,\n",
    "    # Number of folds or the KFold object\n",
    "    cv=skfolds,\n",
    "    # Performance metric\n",
    "    scoring=\"accuracy\",\n",
    "    # Use all computer cores\n",
    "    n_jobs=-1)\n",
    "\n",
    "# Calculate mean and standard deviation for training set scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "\n",
    "# Calculate mean and standard deviation for test set scores\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plot mean accuracy scores for training and test sets\n",
    "plt.plot(param_range, train_mean, label=\"Training score\", color=\"black\")\n",
    "plt.plot(param_range, test_mean, label=\"Cross-validation score\", color=\"dimgrey\")\n",
    "\n",
    "# Plot accurancy bands for training and test sets\n",
    "plt.fill_between(param_range, train_mean - train_std,\n",
    "                 train_mean + train_std, color=\"gray\")\n",
    "plt.fill_between(param_range, test_mean - test_std,\n",
    "                 test_mean + test_std, color=\"gainsboro\")\n",
    "\n",
    "# Create plot\n",
    "plt.title(\"Validation Curve With Random Forest\")\n",
    "plt.xlabel(\"Max depth\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "plt.tight_layout()\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
